{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python3 scripts/alignment/coupe_verset.py --lab ./dataset/english/raw_txt/ --textgrid ./dataset/english/maus_textgrid/ --wav ./dataset/english/wav/ --output ./dataset/allign/ --language English --force --verbose"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python3 coupe_verset.py --lab <CHAPTER TEXT> --textgrid <TEXGRID> --wav <WAV FILES> --output <OUTPUT FOLDER> --language <LANGUAGE ID> --force --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir ../dataset/mass_textgrid/\n",
    "# ! mv ../dataset/wav/*.TextGrid ../dataset/maus_textgrid/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding: utf8 -*-\n",
    "#Collection of supporting functions for the coupe_verset audio slicer\n",
    "#2.0v 15/04/2019 MZ BOITO\n",
    "\n",
    "import glob, os, codecs\n",
    "from praatio import tgio\n",
    "from pprint import pprint\n",
    "\n",
    "class Element():\n",
    "    def __init__ (self, text_key, key, interval):\n",
    "        self.text_key = text_key\n",
    "        self.key = key\n",
    "        self.interval = interval\n",
    "\n",
    "    def to_string(self):\n",
    "        return \" \".join([self.text_key, str(self.interval.start), str(self.interval.end)])\n",
    "    \n",
    "    def _shift_interval(self, interval, value):\n",
    "        if (interval.start - value) < 0 or (interval.end - value) < 0:\n",
    "            raise Exception(\"Invalid value for shift interval function\")\n",
    "        return tgio.Interval(format_number(interval.start - value), format_number(interval.end - value), interval.label)\n",
    "\n",
    "class TextgridWord(Element):\n",
    "    def __init__(self, text_key, key, graphemic, phonetic, phones_list):\n",
    "        Element.__init__(self, text_key, key, graphemic)\n",
    "        self.graphemic = self.interval\n",
    "        self.phonetic = phonetic\n",
    "        self.phones_list = phones_list\n",
    "\n",
    "    def shift_interval(self, value):\n",
    "        self.interval = self._shift_interval(self.interval, value)\n",
    "        self.graphemic = self.interval\n",
    "        self.phonetic = self._shift_interval(self.phonetic, value)\n",
    "        self.phones_list = [self._shift_interval(element, value) for element in self.phones_list]\n",
    "\n",
    "class TextgridSilence(Element):\n",
    "    def __init__(self, text_key, key, interval):\n",
    "        Element.__init__(self, text_key, key, interval)\n",
    "\n",
    "    def shift_interval(self, value):\n",
    "        self.interval = self._shift_interval(self.interval, value)\n",
    "\n",
    "def get_files_list(path):\n",
    "    return glob.glob(path + \"/*\")\n",
    "\n",
    "def get_prefix(file_name):\n",
    "    return file_name.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "def shift_intervals(texgrid_list, value):\n",
    "    for word_obj in texgrid_list:\n",
    "        word_obj.shift_interval(value)\n",
    "\n",
    "def create_textgrid_obj(textgrid_list):\n",
    "    new_dict = dict()\n",
    "    keys = [\"ORT-MAU\", \"KAN-MAU\", \"MAU\"]\n",
    "    for key in keys:\n",
    "        new_dict[key] = tgio.TextgridTier(key, [], 0.0, textgrid_list[-1].interval.end)\n",
    "        new_dict[key].tierType = tgio.INTERVAL_TIER\n",
    "\n",
    "    for element in textgrid_list: \n",
    "        new_dict[\"ORT-MAU\"].entryList.append(element.interval)\n",
    "        try:\n",
    "            phonetic = element.phonetic\n",
    "            phones_list = element.phones_list\n",
    "        except AttributeError:\n",
    "            phonetic = element.interval\n",
    "            phones_list = [element.interval]\n",
    "    \n",
    "        new_dict[\"KAN-MAU\"].entryList.append(phonetic)\n",
    "        new_dict[\"MAU\"].entryList += phones_list\n",
    "\n",
    "    textgrid_obj = tgio.Textgrid()\n",
    "    for key in keys:\n",
    "        textgrid_obj.addTier(new_dict[key])\n",
    "\n",
    "\n",
    "    return textgrid_obj\n",
    "\n",
    "def print_elements_dictionary(elements_dictionary, key):\n",
    "    for element in elements_dictionary[key]:\n",
    "        print(element.to_string())\n",
    "\n",
    "def format_number(float_number):\n",
    "    return float(\"{:.2f}\".format(float_number))\n",
    "\n",
    "def elements_counter(elements_dictionary):\n",
    "    sil = 0\n",
    "    words = 0\n",
    "    for element_list in elements_dictionary.values():\n",
    "        for element in element_list:\n",
    "            try:\n",
    "                element.graphemic\n",
    "                words +=1\n",
    "            except AttributeError:\n",
    "                sil +=1\n",
    "    return words, sil\n",
    "\n",
    "def create_log_file(file_name, dictionary_sequence, textgrid_text):\n",
    "    with codecs.open(file_name, \"w\",\"utf-8\") as log:\n",
    "        log.write(\"{}\\t{}\\n\".format(len(dictionary_sequence.split(\" \")), len(textgrid_text.split(\" \")) ) )\n",
    "        try:\n",
    "            for i in range(len(dictionary_sequence.split(\" \"))):\n",
    "                log.write(\"\\t\".join([dictionary_sequence.split(\" \")[i], textgrid_text.split(\" \")[i]]) + \"\\n\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def check_root(root_directory):\n",
    "    try:\n",
    "        os.stat(root_directory)\n",
    "    except:\n",
    "        os.makedirs(root_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTGRID_SUFFIX = \".TextGrid\"\n",
    "# WAV_SUFFIX = \"_one_channel.wav\"\n",
    "WAV_SUFFIX = \".wav\"\n",
    "SIL_KEY = \"SIL\"\n",
    "SEP_STR = \"_verse_\"\n",
    "langs = [\"en\",\"es\",\"eu\",\"fr\",\"ro\",\"ru\",\"hu\",\"fi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding: utf8 -*-\n",
    "#Collection of cleaning/parsing functions for the coupe_verset audio slicer\n",
    "#2.0v 15/04/2019 MZ BOITO\n",
    "\n",
    "import re, codecs\n",
    "\n",
    "def split_lab(text, language=None):\n",
    "    p_lines = []\n",
    "    regexp = re.compile(r'([0-9]+-[0-9]+)')\n",
    "    number_flag = False\n",
    "    last_number = 0\n",
    "    if language == \"eu\":\n",
    "\n",
    "        if re.compile(r'[0-9]+\\s*-\\s*[0-9]+').search(text):\n",
    "            #print(text)\n",
    "            text = re.sub(r'([0-9]+)\\s*-\\s*([0-9]+)', r'(\\1-\\2)', text) \n",
    "            #text = re.sub(r'(\\D)\\s*-\\s*(\\D)')\n",
    "            #print(text)\n",
    "            #exit(1)\n",
    "\n",
    "    for line in text.split(\"    \"): #4 space\n",
    "#     for line in text.strip().split(): #4 space\n",
    "#         ipdb.set_trace()\n",
    "        if regexp.search(line) and language != \"hu\": #e.g.   (1-39)  text\n",
    "            l = line.split(\"(\")\n",
    "            l1, number, l3 = l[0], l[1].split(\")\")[0], l[1].split(\")\")[1]\n",
    "            number = number.split(\"-\")[-1] #removes first part of \"(START-END)\"\n",
    "            if int(number) > int(last_number):\n",
    "                last_number = number\n",
    "            if not number_flag:\n",
    "                p_lines += [l1, last_number, l3]\n",
    "            else: #adds the number to match transcription\n",
    "                p_lines += [l1, last_number, l[1]]\n",
    "\n",
    "        else:\n",
    "            if line.replace(\" \",\"\").isdigit():\n",
    "                number_flag = True\n",
    "                last_number = line.replace(\" \",\"\")\n",
    "            else:\n",
    "                number_flag = False\n",
    "            p_lines.append(line)\n",
    "    return p_lines\n",
    "\n",
    "def txt_to_dict(txt_path, language=None):\n",
    "    output_dict = dict()\n",
    "    last_key = 0 # zero is the key for the chapter's title\n",
    "    with codecs.open(txt_path, \"r\", \"utf-8\") as txt_file:\n",
    "        for line in txt_file:\n",
    "            for possible_line in split_lab(line, language=language): \n",
    "                line = clean(possible_line,language=language)\n",
    "                \n",
    "#                 ipdb.set_trace()\n",
    "#                 if isinstance(line, int): #verse number\n",
    "#                     last_key = line \n",
    "#                 elif line: #text from the last verse\n",
    "#                     output_dict[last_key] = line\n",
    "                    \n",
    "                output_dict[last_key] = line\n",
    "                last_key += 1\n",
    "    return output_dict\n",
    "\n",
    "def remove_double_space(text, language=None):\n",
    "    if language == \"es\":\n",
    "        split_entry = text.split(\" \")\n",
    "        i  = 0\n",
    "        while(i < len(split_entry)):\n",
    "            if split_entry[i] == '\\xad': #\\xad is a 'soft hyphen', but due to coding problem it is printed as an invisible character\n",
    "                del split_entry[i] \n",
    "            i+=1\n",
    "        text = \" \".join(split_entry)\n",
    "    while \"  \" in text:\n",
    "        text = text.replace(\"  \",\" \")\n",
    "    return text\n",
    "\n",
    "def clean_textgrid(dictionary_case, language):\n",
    "    if language == \"es\" or language == \"hu\":\n",
    "        token = '\\xad' if language == \"es\" else '\\x92'\n",
    "        i  = 0\n",
    "        while(i < len(dictionary_case.entryList)):\n",
    "            if dictionary_case.entryList[i].label == token: \n",
    "                del dictionary_case.entryList[i]\n",
    "            i+=1\n",
    "    return dictionary_case\n",
    "\n",
    "def clean(line, language=None):\n",
    "    marks = [\"“\", \"”\",\"’\"]\n",
    "    punc = [\".\",\"!\",\"?\",\",\"]\n",
    "\n",
    "    if language == \"en\":\n",
    "        line = re.sub(r'(\\D)’s', r'\\1 ’s', line) #space before the apostrophe missing \n",
    "        line = line.replace(\"—\",\" \")\n",
    "        for symbol in [\" \", \".\", \",\", \"?\", \"!\"]:\n",
    "            line = line.replace(\" ’ s\" + symbol, \" ’s\" + symbol)\n",
    "    elif language == \"ru\":\n",
    "        line = line.replace(\"\\'\",\"\").replace(\"--\",\"\")\n",
    "    elif language == \"es\":\n",
    "        line = line.replace(\"»\",\"\").replace(\"«\",\"\").replace(\"–\",\"\").replace('\\xad',\"\")\n",
    "        line = re.sub(r'(\\D)¿(\\D)', r'\\1 ¿\\2', line)\n",
    "    elif language == \"fr\":\n",
    "        line = line.replace(\"»\",\"\").replace(\"«\",\"\").replace(\"–\",\"\").replace('\\xad',\"\").replace(\"…\",\"\")\n",
    "    elif language == \"eu\":\n",
    "        line = line.replace(\"»\",\" \").replace(\"«\",\" \").replace(\"—\",\" \").replace(\"-\",\"\").replace(\":\",\"\").replace(\"/\", \" \").replace(\"…\",\"\") #« between words—\n",
    "    elif language == \"fi\":\n",
    "        line = line.replace(\"-\",\"\").replace(\"‘\",\"\").replace(\":\",\"\")\n",
    "    elif language == \"hu\":\n",
    "        line = line.replace(\"\\\"\",\" \").replace(\":\",\" \").replace(\"-\",\"\").replace(\"\\x92\",\" \").replace(\",\", \" \")\n",
    "    elif language == \"ro\":\n",
    "        line = line.replace(\"–\",\"\").replace(\":\",\"\").replace(\"»\",\"\").replace(\"…\",\"\")\n",
    "\n",
    "    line = re.sub(r',(\\D)', r', \\1', line) #space missing after a comma\n",
    "    line = re.sub(r'(\\D)!(\\D)', r'\\1! \\2', line) #space missing after exclamation point \n",
    "    line = re.sub(r'(\\D)’(\\D)', r'\\1’ \\2', line) #space missing after ending of a quote \n",
    "    line = re.sub(r'(\\D)”(\\D)', r'\\1” \\2', line) #space missing after ending of a quote \n",
    "    line = re.sub(r'(\\D)“(\\D)', r'\\1 “\\2', line) #space missing before beginning of a quote \n",
    "\n",
    "    line = line.replace(\"’ \",\" \")\n",
    "\n",
    "    for symbol in punc + [\")\",\"(\",\";\", \"]\", \"[\"] + marks:\n",
    "        line = line.replace(symbol, \"\")\n",
    "\n",
    "    line = remove_double_space(line)\n",
    "    line = line.replace(\"\\t\",\"\")\n",
    "\n",
    "    if line and line[0] == \" \":\n",
    "        line = line[1:]\n",
    "    if line and line[-1] == \" \":\n",
    "        line = line[:-1]\n",
    "\n",
    "    try:\n",
    "        line = int(line) #verse\n",
    "        return line\n",
    "    except ValueError: #real text\n",
    "        return line.strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding: utf8 -*-\n",
    "# 2.0v last modified the 15/09/2019 MZ BOITO\n",
    "\n",
    "import os, sys, codecs, argparse\n",
    "from praatio import tgio\n",
    "from pprint import pprint\n",
    "# from multiprocessing import Process\n",
    "# from utils import *\n",
    "# from parser import *\n",
    "# from config import langs, TEXTGRID_SUFFIX, WAV_SUFFIX, SIL_KEY, SEP_STR\n",
    "import ipdb\n",
    "\n",
    "from alignment.sequence import Sequence\n",
    "from alignment.vocabulary import Vocabulary\n",
    "from alignment.sequencealigner import SimpleScoring, GlobalSequenceAligner\n",
    "\n",
    "def imperfect_raw_grid_align(dictionary_sequence, textgrid_sequence, verbose=False):\n",
    "    \n",
    "    '''\n",
    "    /!\\ ALLOWING NOT PERFECT ALIGNMENTS \n",
    "    remove \"imperfect_\" from the function name and comment (or remove) the raw_grid_align function\n",
    "    add the following imports at the beginning of this script:\n",
    "\n",
    "    from alignment.sequence import Sequence\n",
    "    from alignment.vocabulary import Vocabulary\n",
    "    from alignment.sequencealigner import SimpleScoring, GlobalSequenceAligner\n",
    "\n",
    "    /!\\ DO NOT USE THE --force option with this function\n",
    "    You might need to remove some asserts (and have some headaches) to make this option work\n",
    "    We do not advise using it\n",
    "    '''\n",
    "#     a = Sequence(dictionary_sequence.split()) #dictionary\n",
    "#     b = Sequence(textgrid_sequence.split()) #textgrid\n",
    "    \n",
    "    a = Sequence(dictionary_sequence.strip().split())\n",
    "    b = Sequence(textgrid_sequence.strip().split())\n",
    "    \n",
    "    \n",
    "#     ipdb.set_trace()\n",
    "    \n",
    "    v = Vocabulary()\n",
    "    aEncoded = v.encodeSequence(a)\n",
    "    bEncoded = v.encodeSequence(b)\n",
    "    # Create a scoring and align the sequences using global aligner.\n",
    "    scoring = SimpleScoring(2, -1)\n",
    "    aligner = GlobalSequenceAligner(scoring, -2)\n",
    "    _, encodeds = aligner.align(aEncoded, bEncoded, backtrace=True)\n",
    "\n",
    "    if not encodeds:\n",
    "        raise Exception(\"Alignment Module failed\")\n",
    "    \n",
    "    # Iterate over optimal alignments, print them if verbose\n",
    "    for encoded in encodeds:\n",
    "        alignment = v.decodeSequenceAlignment(encoded)\n",
    "        if verbose:\n",
    "            for tup in list(alignment):\n",
    "                print(tup)\n",
    "            print ('Alignment score:', alignment.score)\n",
    "            print ('Percent identity:', alignment.percentIdentity())\n",
    "    \n",
    "#     ipdb.set_trace()\n",
    "    return alignment\n",
    "\n",
    "def raw_grid_align(dictionary_sequence, textgrid_sequence, verbose=False):\n",
    "    '''\n",
    "    This function considers perfect textual alignment between chapter \n",
    "    raw text and textgrid (after parser.py cleaning). If not the case, \n",
    "    please check the README and imperfect_raw_grid_alignment for more \n",
    "    information.\n",
    "    '''\n",
    "    \n",
    "    dictionary_sequence = dictionary_sequence.split(\" \")\n",
    "    textgrid_sequence = textgrid_sequence.split(\" \")\n",
    "        \n",
    "#     dictionary_sequence = dictionary_sequence.strip().split()\n",
    "#     textgrid_sequence = textgrid_sequence.strip().split()\n",
    "    \n",
    "    alignment = list()\n",
    "    \n",
    "    for i in range(min(len(textgrid_sequence), len(dictionary_sequence))):\n",
    "        alignment.append((dictionary_sequence[i], textgrid_sequence[i]))\n",
    "            \n",
    "    return alignment\n",
    "\n",
    "#     if (len(textgrid_sequence) != len(dictionary_sequence)):\n",
    "#         ipdb.set_trace()\n",
    "#     add_empty = 0    \n",
    "#         if (len(textgrid_sequence) > len(dictionary_sequence)):\n",
    "#         if (len(dictionary_sequence) > len(textgrid_sequence)):\n",
    "#             add_empty = abs((len(dictionary_sequence) - len(textgrid_sequence)))\n",
    "#     #         for i in range(add_empty):\n",
    "#     #             textgrid_sequence.append(\"\")                    \n",
    "#     for i in range(len(dictionary_sequence)):\n",
    "\n",
    "    \n",
    "\n",
    "def get_tier_by_interval(start, end, tier_dictionary):\n",
    "    return [element for element in tier_dictionary.entryList if element.start >= start and element.end <= end]\n",
    "\n",
    "def get_key_by_index(dictionary, index):\n",
    "    keys = list(dictionary.keys())\n",
    "    key_index = 0\n",
    "    while(index >= 0 and key_index < len(keys)):\n",
    "        line = dictionary[keys[key_index]].split(\" \")\n",
    "        l_length = len(line)\n",
    "        if index >= l_length:\n",
    "            index -= l_length\n",
    "            key_index +=1\n",
    "        else: #index < l_length, the word is at line[index], key is at keys[key_index]\n",
    "            return line[index], keys[key_index]\n",
    "    raise Exception(\"Key not found: Alignment index problem\")\n",
    "\n",
    "def add_time_windows(dictionary, textgrid, alignment):\n",
    "    #ORT-MAU -> words; #KAN-MAU -> phonetic transcription; #MAU -> phoneme alignment\n",
    "    richer_alignment = []\n",
    "    last_verse = 0\n",
    "#     ipdb.set_trace()\n",
    "#     for i in range(len(alignment)):\n",
    "    for i in range(\n",
    "        min(len(alignment), len(textgrid.tierDict[\"ORT-MAU\"].entryList))):\n",
    "        dict_word, tg_word = alignment[i]\n",
    "        try:\n",
    "            word, verse = get_key_by_index(dictionary, i)\n",
    "            last_verse = verse\n",
    "        except Exception: #didn't find the match on the dictionary, uses last alignment found\n",
    "            verse = last_verse\n",
    "\n",
    "        if args.force:\n",
    "            assert word == dict_word, \"Alignment mismatch between the dictionary and the textgrid\"\n",
    "        \n",
    "        graphemic_transcription = textgrid.tierDict[\"ORT-MAU\"].entryList[i]\n",
    "#         graphemic_transcription = textgrid.tierDict[\"ORT\"].entryList[i]\n",
    "\n",
    "        if args.force:\n",
    "            assert graphemic_transcription.label == tg_word, \"Graphemic alignment mismatch\"\n",
    "\n",
    "        phonetic_transcription = get_tier_by_interval(graphemic_transcription.start, graphemic_transcription.end, textgrid.tierDict[\"KAN-MAU\"])[0]\n",
    "        phones_list = get_tier_by_interval(graphemic_transcription.start, graphemic_transcription.end, textgrid.tierDict[\"MAU\"])\n",
    "        tg_word = TextgridWord(tg_word, verse, graphemic_transcription, phonetic_transcription, phones_list)\n",
    "        richer_alignment.append(tg_word)\n",
    "\n",
    "    return richer_alignment\n",
    "\n",
    "def merge_silence(textgrid, alignment):\n",
    "    merged_list = []\n",
    "    silence_list = textgrid.tierDict[\"ORT-MAU\"].getNonEntries()\n",
    "#     silence_list = textgrid.tierDict[\"ORT\"].getNonEntries()\n",
    "    sil_index = 0\n",
    "    text_index = 0\n",
    "    last_verse = 0\n",
    "    while(text_index < len(alignment) or sil_index < len(silence_list)):\n",
    "        if sil_index == len(silence_list): #finished with the silence\n",
    "            merged_list.append(alignment[text_index])\n",
    "            text_index +=1\n",
    "        elif text_index == len(alignment) or alignment[text_index].graphemic.start > silence_list[sil_index].start: \n",
    "            #finished with the text or the silence comes first\n",
    "            sil_obj = TextgridSilence(SIL_KEY,last_verse, silence_list[sil_index])\n",
    "            merged_list.append(sil_obj)\n",
    "            sil_index +=1\n",
    "        else: #word comes first\n",
    "            merged_list.append(alignment[text_index])\n",
    "            last_verse = alignment[text_index].key\n",
    "            text_index +=1\n",
    "    return merged_list\n",
    "\n",
    "def split_by_verse(alignment):\n",
    "    dictionary = dict()\n",
    "    for element in alignment:\n",
    "        try:\n",
    "            dictionary[element.key].append(element)\n",
    "        except KeyError:\n",
    "            dictionary[element.key] = [element]\n",
    "    return dictionary\n",
    "\n",
    "def split_silence(silence_object):\n",
    "    old_interval = silence_object.interval\n",
    "    new_ending = format_number((old_interval.start + (old_interval.end - old_interval.start)/2.0))\n",
    "    new_interval = tgio.Interval(old_interval.start, new_ending, old_interval.label)\n",
    "    new_obj = TextgridSilence(silence_object.text_key, silence_object.key, new_interval)\n",
    "    new_interval = tgio.Interval(new_ending, old_interval.end, old_interval.label)\n",
    "    carry = TextgridSilence(silence_object.text_key, -1, new_interval)\n",
    "    return new_obj, carry\n",
    "\n",
    "def split_boundary_silence(alignment_dictionary):\n",
    "    new_dictionary = dict()\n",
    "    keys = list(alignment_dictionary.keys())\n",
    "    carry = None\n",
    "    for key in keys:\n",
    "        if carry:\n",
    "            carry.key = key\n",
    "            new_dictionary[key] = [carry]\n",
    "            carry = None #consumes carry\n",
    "        else:\n",
    "            new_dictionary[key] = []\n",
    "        \n",
    "        if key == keys[-1]: #last key, nothing to pass for the next \n",
    "            new_dictionary[key] += alignment_dictionary[key]\n",
    "        else:\n",
    "            if alignment_dictionary[key][-1].text_key == SIL_KEY: #if the verse ends with silence\n",
    "                new_dictionary[key] += alignment_dictionary[key][:-1] #everything but the silence goes to the next dictionary\n",
    "                new_obj, carry = split_silence(alignment_dictionary[key][-1])\n",
    "                new_dictionary[key].append(new_obj)\n",
    "\n",
    "            else: #the verse doesn't start or end with silence\n",
    "                new_dictionary[key] += alignment_dictionary[key]\n",
    "\n",
    "    return new_dictionary\n",
    "\n",
    "def align(file, lab_dictionary, grid, verbose=False, language=None):\n",
    "    dictionary_sequence = \" \".join(lab_dictionary.values()) #get the text from the dictionary\n",
    "    tg=tgio.openTextgrid(grid)\n",
    "    \n",
    "    tg.tierDict[\"ORT-MAU\"] = clean_textgrid(tg.tierDict[\"ORT-MAU\"], language) #remove enconding problems for alignment's sake\n",
    "#     tg.tierDict[\"ORT\"] = clean_textgrid(tg.tierDict[\"ORT\"], language) #remove enconding problems for alignment's sake\n",
    "\n",
    "    entryList = tg.tierDict[\"ORT-MAU\"].entryList\n",
    "#     entryList = tg.tierDict[\"ORT\"].entryList\n",
    "    concatenated_ort = \" \".join([entry.label for entry in entryList if entry != \"­\"])\n",
    "    \n",
    "    if args.verbose:\n",
    "        print(\"\\tDICIONARY OUTPUT\")\n",
    "        pprint(lab_dictionary)\n",
    "        print(\"\\tTEXTGRID OUTPUT\")\n",
    "        print(concatenated_ort)\n",
    "\n",
    "    sys.setrecursionlimit(2000) #/!\\ this might be a problem\n",
    "#     split_entry = concatenated_ort.split(\" \")\n",
    "    split_entry = concatenated_ort.strip().split(\" \")\n",
    "\n",
    "    if args.force:\n",
    "        try:\n",
    "            assert len(dictionary_sequence.split(\" \")) == len(split_entry), \"Number of words mismatch between lab and textgrid\"\n",
    "        except AssertionError:\n",
    "            create_log_file(file.split(\"/\")[-1] + \"_error_log\", dictionary_sequence, concatenated_ort)\n",
    "            exit(1)\n",
    "    \n",
    "    \n",
    "    alignment = raw_grid_align(dictionary_sequence, concatenated_ort)\n",
    "#     alignment = imperfect_raw_grid_align(dictionary_sequence, concatenated_ort)\n",
    "#     ipdb.set_trace()\n",
    "    \n",
    "    if args.force:\n",
    "        assert len(alignment) == len(dictionary_sequence.split(\" \")), \"Number of words mismatch between final alignment and dictionary\"\n",
    "    \n",
    "#     ipdb.set_trace()\n",
    "    richer_alignment = add_time_windows(lab_dictionary, tg, alignment)\n",
    "    complete_alignment = merge_silence(tg, richer_alignment)\n",
    "    splitted_alignment = split_by_verse(complete_alignment)\n",
    "    final_alignment = split_boundary_silence(splitted_alignment)\n",
    "\n",
    "    words, sil = elements_counter(final_alignment)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Final alignment has %d words and %d silence marks\" % (words, sil))\n",
    "    \n",
    "    assert words == len(alignment), \"The script lost part of the words during the alignment\"\n",
    "    \n",
    "    return final_alignment\n",
    "\n",
    "def generate_audio_cuts(alignment_dictionary):\n",
    "    return [(key, alignment_dictionary[key][0].interval.start, alignment_dictionary[key][-1].interval.end) for key in alignment_dictionary.keys()]\n",
    "\n",
    "def slice_audio(audio, output_prefix, windows, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"Cutting audio %s\" % (audio))\n",
    "    for (key, start, end) in windows:\n",
    "        output_file = output_prefix + SEP_STR + str(key) + WAV_SUFFIX\n",
    "        if verbose:\n",
    "            print(output_file, start, end)\n",
    "#         ipdb.set_trace()\n",
    "        os.system(\"sox {} {} trim {} ={}\".format(audio, output_file, start, end))\n",
    "\n",
    "def write_new_textgrids(output_prefix, windows, alignment_dictionary):\n",
    "    assert len(windows) == len(alignment_dictionary.keys()), \"Size Mismatch between audio windows and textgrids\"\n",
    "    for (key, start, _) in windows:\n",
    "        if start != 0:\n",
    "            shift_intervals(alignment_dictionary[key], start)\n",
    "        \n",
    "#         ipdb.set_trace()\n",
    "        \n",
    "        obj = create_textgrid_obj(alignment_dictionary[key])\n",
    "        output_file = output_prefix + SEP_STR + str(key) + TEXTGRID_SUFFIX\n",
    "        obj.save(output_file)\n",
    "\n",
    "def write_text_files(output_prefix, lab_dictionary):\n",
    "    for key in lab_dictionary.keys():\n",
    "        output_file = output_prefix + SEP_STR + str(key) + \".txt\"\n",
    "        with codecs.open(output_file, \"w\",\"utf-8\") as output_file:\n",
    "            output_file.write(lab_dictionary[key] + \"\\n\")\n",
    "\n",
    "def process_document(lab_file, args):\n",
    "    if args.verbose:\n",
    "            print(lab_file)\n",
    "    \n",
    "    # we get the prefix of the file (e.g 'B05___05_Acts________ENGESVN1DA')\n",
    "    file_prefix = get_prefix(lab_file)\n",
    "    \n",
    "    # we get the text file in dictionary form {0: 'Acts 5', 1:'But a man ...' ...}\n",
    "    lab_dictionary = txt_to_dict(lab_file, args.language)\n",
    "    \n",
    "    # get the the texgrid file conresponding to the 'file_prefix'\n",
    "    textgrid_file = os.path.join(args.textgrid, file_prefix + TEXTGRID_SUFFIX)\n",
    "    \n",
    "#     ipdb.set_trace()\n",
    "    \n",
    "    # \n",
    "    alignment_dictionary = align(lab_file, lab_dictionary, textgrid_file, language=args.language, verbose= args.verbose)\n",
    "    \n",
    "    # \n",
    "    windows = generate_audio_cuts(alignment_dictionary)\n",
    "    \n",
    "    output_prefix = os.path.join(args.output, file_prefix) \n",
    "    slice_audio(os.path.join(args.wav,  file_prefix + WAV_SUFFIX), output_prefix, windows, verbose=args.verbose)\n",
    "    \n",
    "    write_new_textgrids(output_prefix, windows, alignment_dictionary)\n",
    "    write_text_files(output_prefix, lab_dictionary)\n",
    "\n",
    "def process(args):   \n",
    "    labs = get_files_list(args.lab)\n",
    "    textgrids = get_files_list(args.textgrid)\n",
    "    wavs = get_files_list(args.wav)\n",
    "\n",
    "    assert len(labs) == len(textgrids) and len(textgrids) == len(wavs), \"Different number of files inside the folders\"\n",
    "\n",
    "    for lab_file in labs:\n",
    "        # 1) REMOVE THE COMMENT BELOW TO REMOVE MULTIPROCESSING\n",
    "        process_document(lab_file, args)\n",
    "        # 2) COMMENT THE FOLLOWING 3 LINES TO REMOVE MULTIPROCESSING\n",
    "        #p = Process(target=process_document, args=(lab_file, args))\n",
    "        #p.start()\n",
    "    #p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-47e89cd269ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mcheck_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-3cb1874d89ed>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlab_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;31m# 1) REMOVE THE COMMENT BELOW TO REMOVE MULTIPROCESSING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mprocess_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;31m# 2) COMMENT THE FOLLOWING 3 LINES TO REMOVE MULTIPROCESSING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m#p = Process(target=process_document, args=(lab_file, args))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3cb1874d89ed>\u001b[0m in \u001b[0;36mprocess_document\u001b[0;34m(lab_file, args)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;31m# we get the text file in dictionary form {0: 'Acts 5', 1:'But a man ...' ...}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mlab_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxt_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;31m# get the the texgrid file conresponding to the 'file_prefix'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a1b37cbed447>\u001b[0m in \u001b[0;36mtxt_to_dict\u001b[0;34m(txt_path, language)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtxt_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtxt_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mpossible_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_lab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossible_line\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a1b37cbed447>\u001b[0m in \u001b[0;36msplit_lab\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"hu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#e.g.   (1-39)  text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"(\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\")\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\")\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#removes first part of \"(START-END)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--lab', type=str, nargs='?', help='lab folder')\n",
    "    parser.add_argument('--textgrid', type=str, nargs='?', help='textgrid folder')\n",
    "    parser.add_argument('--wav', type=str, nargs='?', help='wav folder')\n",
    "    parser.add_argument(\"--verbose\", \"-v\", help=\"increases output verbosity\", action=\"store_true\")\n",
    "    parser.add_argument(\"--force\", \"-f\", help=\"forces a perfect alignment between textgrid and lab\", action=\"store_true\")\n",
    "    parser.add_argument('--output', type=str, nargs='?', help=\"name for the output folder\")\n",
    "    parser.add_argument('--language', type=str, nargs='?', help='specifies language for cleaning and alignment')\n",
    "    \n",
    "    # https://github.com/spyder-ide/spyder/issues/3883\n",
    "    import sys\n",
    "    sys.argv=['']#; del sys \n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    args.lab = \"../dataset/english/\"\n",
    "    args.textgrid = \"../dataset/maus_textgrid/\"\n",
    "    args.wav = \"../dataset/wav/\"\n",
    "    args.output = \"../dataset/allign/\"\n",
    "    args.language = \"English\"\n",
    "    args.verbose = False\n",
    "    args.force = False\n",
    "    \n",
    "#     args.lab = \"../dataset_old/english/\"\n",
    "#     args.textgrid = \"../dataset_old/textgrid/\"\n",
    "#     args.wav = \"../dataset_old/wav_verse/\"\n",
    "#     args.output = \"../dataset_old/allign/\"\n",
    "#     args.language = \"English\"\n",
    "#     args.verbose = True\n",
    "#     args.force = True\n",
    "        \n",
    "    if not (args.lab and args.textgrid and args.wav and args.output):\n",
    "        parser.print_help()\n",
    "        print(\"LIST OF SUPPORTED LANGUAGES: %s\" % (\" \".join(langs)))\n",
    "        exit(1)\n",
    "    \n",
    "    check_root(args.output)\n",
    "    process(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move the splited speech and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../dataset/wav_verse’: File exists\n",
      "mkdir: cannot create directory ‘../dataset/English’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir ../dataset/wav_verse\n",
    "!mkdir ../dataset/English\n",
    "\n",
    "!mv ../dataset/allign/*.wav ../dataset/wav_verse/\n",
    "!mv ../dataset/allign/*.txt ../dataset/English/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Old : \n",
    "\n",
    "{'ORT': <praatio.tgio.IntervalTier object at 0x7fee78443278>, 'KAN': <praatio.tgio.IntervalTier object at 0x7fee78443ac8>, 'MAU': <praatio.tgio.IntervalTier object at 0x7fee7820b0f0>}\n",
    "\n",
    "\n",
    "New Maus output : \n",
    "\n",
    "{'ORT-MAU': <praatio.tgio.IntervalTier object at 0x7fee7836fd30>, 'KAN-MAU': <praatio.tgio.IntervalTier object at 0x7fee78397eb8>, 'MAU': <praatio.tgio.IntervalTier object at 0x7fee78369400>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
